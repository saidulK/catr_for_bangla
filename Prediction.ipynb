{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from models import  caption\n",
    "from datasets import coco\n",
    "from configuration import Config\n",
    "from engine import evaluate\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from configuration import Config\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from models import caption\n",
    "from datasets import coco, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = '..\\\\Bangla Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_location = '../checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion = caption.build_model(Config())\n",
    "model.load_state_dict(torch.load(checkpoint_location)['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../Bangla Dataset/images/54.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "start_token = tokenizer.convert_tokens_to_ids(tokenizer._cls_token)\n",
    "end_token = tokenizer.convert_tokens_to_ids(tokenizer._sep_token)\n",
    "\n",
    "img = Image.open(image_path)\n",
    "image = coco.val_transform(img)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "def create_caption_and_mask(start_token, max_length):\n",
    "    caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "    mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "\n",
    "    caption_template[:, 0] = start_token\n",
    "    mask_template[:, 0] = False\n",
    "\n",
    "    return caption_template, mask_template\n",
    "\n",
    "\n",
    "caption, cap_mask = create_caption_and_mask(\n",
    "    start_token, Config().max_position_embeddings)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    for i in range(Config().max_position_embeddings - 1):\n",
    "        predictions = model(image, caption, cap_mask)\n",
    "        predictions = predictions[:, i, :]\n",
    "        predicted_id = torch.argmax(predictions, axis=-1)\n",
    "\n",
    "        if predicted_id[0] == 102:\n",
    "            return caption\n",
    "\n",
    "        caption[:, i+1] = predicted_id[0]\n",
    "        cap_mask[:, i+1] = False\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "output = evaluate()\n",
    "result = tokenizer.decode(output[0].tolist(),skip_special_tokens=True)\n",
    "#result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(result)\n",
    "\n",
    "display(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "035b8b1ad6bbe0a6b9c859f6e8d3b4b6764cd894bf20b06a8ccd613eccc598d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
